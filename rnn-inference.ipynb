{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport matplotlib.pyplot as plt \nimport torch.nn as nn\nimport torch\nimport string","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-18T17:18:54.591293Z","iopub.execute_input":"2023-04-18T17:18:54.591714Z","iopub.status.idle":"2023-04-18T17:18:54.597911Z","shell.execute_reply.started":"2023-04-18T17:18:54.591676Z","shell.execute_reply":"2023-04-18T17:18:54.596563Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def rep_name(word):\n    # creating a tensor of zeros to make the embeddings for word parameters\n    # len(word)---> word length\n    # 1 where we want to put the 1 in the vector that will made by the word representation\n    # n_letters --> all the letters in vocab\n    ten=torch.zeros(len(word),1,n_letters)\n    \n    for i , ch in enumerate(word):\n        #position of a letter in the all letters\n        pos=all_letters.find(ch)\n        #ten[i][0][pos]----->ten[i]-->len(word) \n        #ten[i][0]---> inner dim list \n        #ten[i][0][pos]--> where position of the letter make it 1\n        ten[i][0][pos]=1\n        # return it \n        return ten\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T17:18:54.905847Z","iopub.execute_input":"2023-04-18T17:18:54.907056Z","iopub.status.idle":"2023-04-18T17:18:54.914902Z","shell.execute_reply.started":"2023-04-18T17:18:54.906996Z","shell.execute_reply":"2023-04-18T17:18:54.913823Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\nall_letters=string.ascii_letters\nn_letters=len(all_letters)\nn_letters","metadata":{"execution":{"iopub.status.busy":"2023-04-18T17:19:06.891969Z","iopub.execute_input":"2023-04-18T17:19:06.892358Z","iopub.status.idle":"2023-04-18T17:19:06.902938Z","shell.execute_reply.started":"2023-04-18T17:19:06.892324Z","shell.execute_reply":"2023-04-18T17:19:06.900608Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"52"},"metadata":{}}]},{"cell_type":"code","source":"class RNN_net(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN_net, self).__init__()\n        #this is s_i\n        #initializing this beacuse we need previous hidden_state to find the current hidden state(hidden_state-1)\n        self.hidden_size = hidden_size\n        # creating the hidden state layer \n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)# U and W\n        #creating the output_layer \n        #O(weights *current_hidden_state)+c\n        self.i2o = nn.Linear(input_size + hidden_size, output_size)#V\n        #creating output using softmax\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input_, hidden):   \n        #combining input +hidden \n        combined = torch.cat((input_, hidden), 1)\n        #print('Combined :',combined.shape)\n        #creating th hidden state\n        hidden = self.i2h(combined)\n        #print('Hidden(i2h) :',hidden.shape)\n        #creating output-vector\n        output = self.i2o(combined)\n        #print('Output(i2o):',output.shape)\n        #predicting prob\n        output = self.softmax(output)\n        #print('Output softmax :',output.shape)\n        return output, hidden\n\n    def init_hidden(self):\n        x= torch.zeros(1, self.hidden_size)\n        print(f'Hidden initialize: ')\n        print(f'\\n\\033[1m\\033[36m\\033[4m s(0) = {x.shape} \\033[0m')\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-18T18:52:21.682436Z","iopub.execute_input":"2023-04-18T18:52:21.682843Z","iopub.status.idle":"2023-04-18T18:52:21.693529Z","shell.execute_reply.started":"2023-04-18T18:52:21.682808Z","shell.execute_reply":"2023-04-18T18:52:21.691991Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"markdown","source":"1. ## Hidden state : $s_i = \\sigma(U{x_i} + W_{s_i-1} + b)$\n2. ## Output state : $y_i= O(V{s_i}+c)$\n\n","metadata":{}},{"cell_type":"code","source":"\nn_hidden = 2\nnet = RNN_net(n_letters, n_hidden,1)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T18:52:22.402898Z","iopub.execute_input":"2023-04-18T18:52:22.403282Z","iopub.status.idle":"2023-04-18T18:52:22.409735Z","shell.execute_reply.started":"2023-04-18T18:52:22.403250Z","shell.execute_reply":"2023-04-18T18:52:22.408021Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"def infer(net, name):\n    net.eval()\n    name_ohe = rep_name(name)\n    hidden = net.init_hidden()\n    \n    for x in range(name_ohe.size()[0]):\n        print('\\nTime stamp : ',x+1)\n        print(f'\\nInput_shape : X{x+1} = {name_ohe[0].shape}')\n        output, hidden = net(name_ohe[x], hidden)\n        print(f'Hidden : S{x+1} = {hidden.shape}')\n        print(f'Output : Y{x+1} = {output.shape}')\n        \n        print(f'\\n\\033[1m\\033[36m\\033[4ms({x+1})  =  Sigma( Ux{(x+1)} +  Ws({x})  +  b )\\033[0m')\n        \n        print(f'\\n\\033[1m\\033[36m\\033[4my({x+1})  =  O( Vs({x+1}))  +  c )\\033[0m')\n        \n        \n       \n\n        \n    \n    return output\n\noutput = infer(net, 'shiv')\nindex = torch.argmax(output)\nprint('\\nFinal Output')\nprint(output.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T18:52:22.870928Z","iopub.execute_input":"2023-04-18T18:52:22.871307Z","iopub.status.idle":"2023-04-18T18:52:22.882421Z","shell.execute_reply.started":"2023-04-18T18:52:22.871275Z","shell.execute_reply":"2023-04-18T18:52:22.881200Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"Hidden initialize: \n\n\u001b[1m\u001b[36m\u001b[4m s(0) = torch.Size([1, 2]) \u001b[0m\n\nTime stamp :  1\n\nInput_shape : X1 = torch.Size([1, 52])\nHidden : S1 = torch.Size([1, 2])\nOutput : Y1 = torch.Size([1, 1])\n\n\u001b[1m\u001b[36m\u001b[4ms(1)  =  Sigma( Ux1 +  Ws(0)  +  b )\u001b[0m\n\n\u001b[1m\u001b[36m\u001b[4my(1)  =  O( Vs(1))  +  c )\u001b[0m\n\nTime stamp :  2\n\nInput_shape : X2 = torch.Size([1, 52])\nHidden : S2 = torch.Size([1, 2])\nOutput : Y2 = torch.Size([1, 1])\n\n\u001b[1m\u001b[36m\u001b[4ms(2)  =  Sigma( Ux2 +  Ws(1)  +  b )\u001b[0m\n\n\u001b[1m\u001b[36m\u001b[4my(2)  =  O( Vs(2))  +  c )\u001b[0m\n\nTime stamp :  3\n\nInput_shape : X3 = torch.Size([1, 52])\nHidden : S3 = torch.Size([1, 2])\nOutput : Y3 = torch.Size([1, 1])\n\n\u001b[1m\u001b[36m\u001b[4ms(3)  =  Sigma( Ux3 +  Ws(2)  +  b )\u001b[0m\n\n\u001b[1m\u001b[36m\u001b[4my(3)  =  O( Vs(3))  +  c )\u001b[0m\n\nTime stamp :  4\n\nInput_shape : X4 = torch.Size([1, 52])\nHidden : S4 = torch.Size([1, 2])\nOutput : Y4 = torch.Size([1, 1])\n\n\u001b[1m\u001b[36m\u001b[4ms(4)  =  Sigma( Ux4 +  Ws(3)  +  b )\u001b[0m\n\n\u001b[1m\u001b[36m\u001b[4my(4)  =  O( Vs(4))  +  c )\u001b[0m\n\nFinal Output\ntorch.Size([1, 1])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}